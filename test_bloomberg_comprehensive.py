#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•å½­åšç¤¾æ–°é—»æŠ“å–å™¨
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.fetchers.bloomberg import BloombergFetcher
from src.utils.logger import logger
import time


def test_bloomberg_fetcher_comprehensive():
    """å…¨é¢æµ‹è¯•å½­åšç¤¾æŠ“å–å™¨"""
    print("="*60)
    print("å¼€å§‹å…¨é¢æµ‹è¯•å½­åšç¤¾æ–°é—»æŠ“å–å™¨...")
    print("="*60)
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
    fetcher = BloombergFetcher()
    
    try:
        print(f"æ­£åœ¨è¿æ¥åˆ°: {fetcher.base_url}")
        print(f"ä½¿ç”¨ User-Agent: {fetcher.session.headers.get('User-Agent', 'Default')}")
        print()
        
        # è¿è¡ŒæŠ“å–
        start_time = time.time()
        articles = fetcher.run()
        end_time = time.time()
        
        print(f"âœ… æˆåŠŸæŠ“å–åˆ° {len(articles)} ç¯‡æ–°é—»")
        print(f"â±ï¸  æŠ“å–è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        if articles:
            print("\nğŸ“ˆ æŠ“å–ç»Ÿè®¡:")
            categories = {}
            for article in articles:
                cat = article.get('category', 'æœªçŸ¥')
                categories[cat] = categories.get(cat, 0) + 1
            
            for cat, count in categories.items():
                print(f"  {cat}: {count} ç¯‡")
        
        # æ˜¾ç¤ºå‰å‡ ç¯‡æ–°é—»çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ“° å‰ 5 ç¯‡æ–°é—»è¯¦æƒ…:")
        print("-"*60)
        
        for i, article in enumerate(articles[:5]):
            print(f"\nã€ç¬¬ {i+1} ç¯‡ã€‘")
            print(f"æ ‡é¢˜: {article.get('title', 'N/A')}")
            print(f"é“¾æ¥: {article.get('url', 'N/A')}")
            print(f"åˆ†ç±»: {article.get('category', 'N/A')}")
            print(f"æ—¶é—´: {article.get('published_at', 'N/A')}")
            print(f"ä¼˜å…ˆçº§: {article.get('priority', 'N/A')}")
            print(f"æ ‡ç­¾: {', '.join(article.get('tags', []))}")
            
            content_preview = article.get('content', '')[:200] + "..." if len(article.get('content', '')) > 200 else article.get('content', '')
            print(f"å†…å®¹é¢„è§ˆ: {content_preview}")
        
        print("\n" + "="*60)
        print("âœ… å½­åšç¤¾æŠ“å–å™¨æµ‹è¯•å®Œæˆ!")
        print("="*60)
        
        return articles
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å½­åšç¤¾æŠ“å–å™¨æ—¶å‡ºé”™: {e}", exc_info=True)
        return []


def test_individual_sources():
    """æµ‹è¯•å„ä¸ªRSSæº"""
    print("\nğŸ” æµ‹è¯•å„ä¸ªRSSæº...")
    
    fetcher = BloombergFetcher()
    
    for category, feed_url in fetcher.RSS_FEEDS.items():
        print(f"\næµ‹è¯• {category} æº: {feed_url}")
        try:
            import feedparser
            feed = feedparser.parse(feed_url)
            print(f"  âœ… æˆåŠŸè¿æ¥ï¼Œè·å–åˆ° {len(feed.entries)} ä¸ªé¡¹ç›®")
            if feed.bozo:
                print(f"  âš ï¸  è§£æè­¦å‘Š: {feed.bozo_exception}")
        except Exception as e:
            print(f"  âŒ è¿æ¥å¤±è´¥: {e}")


if __name__ == "__main__":
    articles = test_bloomberg_fetcher_comprehensive()
    
    # é¢å¤–æµ‹è¯•å„ä¸ªRSSæº
    test_individual_sources()
    
    print(f"\næ€»ç»“: å…±è·å– {len(articles)} ç¯‡æ–°é—»")
    
    # å¦‚æœéœ€è¦é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥æ‰§è¡Œç‰¹å®šåŠŸèƒ½ï¼Œå¯ä»¥æ·»åŠ å¦‚ä¸‹ä»£ç 
    if len(sys.argv) > 1:
        if sys.argv[1] == "validate":
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ éªŒè¯åŠŸèƒ½
            print("\néªŒè¯åŠŸèƒ½å¾…å®ç°...")